---
title: ""
author: ""
date: ""
output: html_document
---

### A sample analysis (complete with fake data)

What we can achieve from EFA:

1. Explore the **actual correlations** between our items; not just theorectical correlations (some questionnaire items might end up capturing none or more than one trait and we can't perform hypothesis testing with that item)
2. It allows us to trim away some items or construct new factors (traits) to explain our data
3. Sometime there are traits that are not measured at all properly by all our survey questions (and we have to add some)

- General workflow:
1. PCA screening
2. EFA
3. Regression modeling

#### Generate random survey data (n=500)

- Q1-3 are highly correlated (pearson's r > 0.70)
- Q3-6 are moderately correlated (r < 0.60)
- Q7-9 are noise (r ~ 0)
- Q10-12 are highly correlated (r > 0.70)
- Q13-15 are moderately correlated (r < 0.60)

1. The data is designed such that Q1-6 measure one component (e.g. Knowledge) and Q10-15 measure another component (e.g. Perceptions). 
2. The first 3 questions in each group are high quality and correlate highly with each other
3. The next 3 questions in each group are lower quality and only moderately correlated with each other
4. Q7-9 are pure noise and represent very poorly designed variables

```{r, echo=FALSE, message=FALSE, warning=FALSE}
mod.cor <- function(x1,r=0.6, n=500) {                    # length of vector
        rho   <- r                   # desired correlation = cos(angle)
        theta <- acos(rho)             # corresponding angle      # fixed given data
        x2    <- rnorm(n, 2, 0.5)      # new random data
        X     <- cbind(x1, x2)         # matrix
        Xctr  <- scale(X, center=TRUE, scale=FALSE)   # centered columns (mean 0)

        Id   <- diag(n)                               # identity matrix
        Q    <- qr.Q(qr(Xctr[ , 1, drop=FALSE]))      # QR-decomposition, just matrix Q
        P    <- tcrossprod(Q)          # = Q Q'       # projection onto space defined by x1
        x2o  <- (Id-P) %*% Xctr[ , 2]                 # x2ctr made orthogonal to x1ctr
        Xc2  <- cbind(Xctr[ , 1], x2o)                # bind to matrix
        Y    <- Xc2 %*% diag(1/sqrt(colSums(Xc2^2)))  # scale columns to length 1

        x3 <- Y[ , 2] + (1 / tan(theta)) * Y[ , 1]     # final new vector
        x3 <- x3 * 10 * 2
        return(round(x3,digits=0))
}
                                 # check correlation = rho

gen.data <- function(seed) {
        set.seed(seed)
        x <- sample(1:3, 500, replace=TRUE)
        x2 <- x - sample(1:2, 500, replace=TRUE)
        x3 <- x - sample(1:2, 500, replace=TRUE)
        x4 <- mod.cor(x,r=0.5)
        x5 <- mod.cor(x,r=0.49)
        x6 <- mod.cor(x,r=0.48)

        z <- sample(-3:3, 500, replace=TRUE) ####Noise
        z2 <- sample(-3:3, 500, replace=TRUE)
        z3 <- sample(-3:3, 500, replace=TRUE)
        
        y <- sample(-3:-1, 500, replace=TRUE) 
        y2 <- y + sample(1:2, 500, replace=TRUE)
        y3 <- y + sample(1:2, 500, replace=TRUE)
        y4 <- mod.cor(y, r=0.50)
        y5 <- mod.cor(y, r=0.51)
        y6 <- mod.cor(y, r=0.53)
        
        q <- -3:3
        data <- as.data.frame(list(Q1=factor(x,levels=levels(factor(q))),
                                Q2=factor(x2,levels=levels(factor(q))),
                                Q3=factor(x3,levels=levels(factor(q))),
                                Q4=factor(x4,levels=levels(factor(q))),
                                Q5=factor(x5,levels=levels(factor(q))),
                                Q6=factor(x6,levels=levels(factor(q))),
                                Q7=factor(z,levels=levels(factor(q))),
                                Q8=factor(z2,levels=levels(factor(q))),
                                Q9=factor(z3,levels=levels(factor(q))),
                                Q10=factor(y,levels=levels(factor(q))),
                                Q11=factor(y2,levels=levels(factor(q))),
                                Q12=factor(y3,levels=levels(factor(q))),
                                Q13=factor(y4,levels=levels(factor(q))),
                                Q14=factor(y5,levels=levels(factor(q))),
                                Q15=factor(y6,levels=levels(factor(q)))))
        data                   
}
data <- gen.data(1)
```

#### View correlation matrix

As terms a non-linear (i.e. likert data), we will use a **polychoric correlation matrix**

```{r, echo=FALSE, warning=FALSE}
library(polycor)
hetcor(data,ML=F,std.err=F)
```

#### Sample a training set (n=200)

```{r, echo=FALSE}
index <- sample(nrow(data), 200)
train <- data[index, ]
test <- data[-index,]
```

#### Step 1a. Generate items

- Create items (i.e. Questions) that supposedly tap into the target construct
- Questions should be as expansive as possible: maximize face value
- Redundancy at this stage is fine but missing items is serious

#### Step 1b Screening items

Non-linear principal components analysis with Promax rotation is performed:

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(psych)
train.cor <- hetcor(train,ML=F,std.err=F)
#### Run PCA
pca <- principal(train.cor$correlations, nfactors=10, rotate="promax")
pca
```

#### Step 1b Screening items (cont)

- We look for variables that load clearly onto one component
- Try to minimize cross-loading: i.e. high loadings on >1 component
- A cut-off that is generally accepted (but should be reported)
1. `>`0.50-0.60 on main factor and <0.20-0.30 on others
2. We can discard questions that don't meet this criteria
- *Make sure that the results make theorectical sense!*

#### Step 2. Exploratory Factor Analysis using test data (n=300)

1. Determine number of factors underlying the data
2. Identify which variables load on to which factors
3. Remove variables which do not load onto any factors

*New data should be used*, we use that data that was not used just now (n=300)

#### Step 2. EFA: Parallel analysis

```{r,echo=FALSE,message=FALSE, warning=FALSE}
test.cor <- hetcor(test,ML=F,std.err=F)
options(mc.cores=8)
start.time <- Sys.time()
test.parallel <- fa.parallel.poly(test.cor$correlations, global=F)
end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
```


#### Step 2. EFA: Parallel analysis (cont.)

1. 500-1000 artificial datasets are generated with the same number of variables and same number of observations
2. Variables in these datasets are random variables
3. Factor analysis is performed on each of these datasets and the eigenvalues of each factor generated are extracted
3. Repeating this many times will allow us to calculate the mean eigenvalue of a factor from datasets similar to what we have
4. An eigenvalue less than the mean would mean that a particular factor extracted is no more substantial (explains less) than a random factor (which really has no meaning); those factors are rejected 

#### Step 2. EFA: Parallel analysis (cont.)

Parallel analysis suggests **`r test.parallel$nfact` factors** are to be extracted

#### Step 2. EFA: Factor analysis with the number of factors selected by PA

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library("GPArotation")
test.fa <- fa(r=test.cor$correlations,nfactors=test.parallel$nfact,rotate="promax")
test.fa
```


#### Step 2. EFA: Screening items

This step is identical to that performed during PCA

- We look for factors that load clearly onto one component
- Try to minimize cross-loading: i.e. high loadings on >1 component
- A cut-off that is generally accepted (but should be reported)
1. `>`0.50-0.60 on main factor and <0.20-0.30 on others
2. We can discard questions that don't meet this criteria
- *Make sure that the results make theorectical sense!*

#### Step 3. Creating factor scores

- Methods to create factor scores can be divided into non-refined and refined.
- The latter combines PCA and common factor extraction methods to maximize validity by creating scores that are
1. Highly correlated with a given factor
2. Obtain unbiased estimates of a true factor score
3. Preserves relationships between factors

#### Step 3. Refined factor score computation

- Three main methods exist:
1. Regression Scores
2. Bartlett Scores
3. Anderson-Rubin Scores

#### Step 4. Determining the validity of the factor scores computed

- Grice (2001) suggests researchers examine the degree of indeterminacy in their factor solutions using three different measures:
1. validity – evidence of correlational relationships between factor scores and factor(s)
2. univocality- the extent to which factor scores are adequately or insufficiently correlated with other factors in the same analysis
3. correlational accuracy – which reports the extent to which correlations among the estimated factor scores match the correlations among the factors themselves. 

**The Bartlett score was used with the above factor analysis**
```{r, echo=TRUE}
bartlett.scores <- factor.scores(data.matrix(test),f=test.fa,method="Bartlett")$scores

test.wscores <- cbind(test,bartlett.scores)
```



#### Step 5. Using the factor scores for regression

- Factors are new variables!
- They must be checked for assumptions (e.g. normality) before conducting hypothesis testing!

